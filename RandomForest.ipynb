{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RandomForest.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/nenorden/Transfer-learning-on-Fashion-MNIST/blob/master/RandomForest.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "du6qu80fpl6C",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Download Fashion-MNIST"
      ]
    },
    {
      "metadata": {
        "id": "U5tsOuhzqGKj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "c9872fc8-1811-4d77-bd9c-6c05bedacb5d"
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('/content/')\n",
        "!git clone https://github.com/zalandoresearch/fashion-mnist.git \n",
        "os.chdir('/content/fashion-mnist/')\n",
        "import utils.mnist_reader as mnist_reader\n",
        "os.chdir(\"/\")\n",
        "train_x_orig, train_y = mnist_reader.load_mnist('content/fashion-mnist/data/fashion', kind='train')\n",
        "test_x_orig, test_y = mnist_reader.load_mnist('content/fashion-mnist/data/fashion', kind='t10k')\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'fashion-mnist'...\n",
            "remote: Counting objects: 615, done.\u001b[K\n",
            "remote: Compressing objects: 100% (8/8), done.\u001b[K\n",
            "remote: Total 615 (delta 5), reused 9 (delta 4), pack-reused 603\u001b[K\n",
            "Receiving objects: 100% (615/615), 105.18 MiB | 56.24 MiB/s, done.\n",
            "Resolving deltas: 100% (347/347), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "K477rMNaoY0c",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Import ResNet Features"
      ]
    },
    {
      "metadata": {
        "id": "M4-LDkpigx6f",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Download data from Drive to virtual machine"
      ]
    },
    {
      "metadata": {
        "id": "Kb_APrWbogR_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "eb030d14-8f45-404e-f444-d72886261a16"
      },
      "cell_type": "code",
      "source": [
        "!pip install -U -q PyDrive\n",
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# Eliza's file id's\n",
        "#train_file_id = '1j1488T_da1oVlwwy-_19GMuY9tSucqzg'\n",
        "#test_file_id = '1qR7fOsQ6TTW8eV8SnKN-jVvswlzMx9QM'\n",
        "\n",
        "# Alex' file id's\n",
        "train_file_id = '1u9MTHtqwDtHL3z6NHMP8xRJw_54Fag5T'\n",
        "test_file_id = '1kkbjDm6hilfA4PFcxBgRGrN8gcGY6dmn'\n",
        "\n",
        "os.chdir('/content/')\n",
        "# Load files by ID\n",
        "downloaded = drive.CreateFile({'id': train_file_id})\n",
        "downloaded.GetContentFile('train_fashion-MNIST_resnet.npy')\n",
        "\n",
        "downloaded = drive.CreateFile({'id': test_file_id})\n",
        "downloaded.GetContentFile('test_fashion-MNIST_resnet.npy')\n",
        "\n",
        "# Files should be in /content/. Check!\n",
        "\n",
        "!ls\n",
        "os.chdir('/')\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "datalab        test_fashion-MNIST_resnet.npy\r\n",
            "fashion-mnist  train_fashion-MNIST_resnet.npy\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "j7atbqx1g2A5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Load ResNet data"
      ]
    },
    {
      "metadata": {
        "id": "bt8XQJI6oL1w",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import tensorflow as tf\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "os.chdir('/content/')\n",
        "train_x = np.load('train_fashion-MNIST_resnet.npy')\n",
        "test_x = np.load('test_fashion-MNIST_resnet.npy')\n",
        "os.chdir('/')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fgEnDvEd3yJm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Random Forest"
      ]
    },
    {
      "metadata": {
        "id": "oKga9MT7v86w",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## ResNet features"
      ]
    },
    {
      "metadata": {
        "id": "9SOgNbhv3zYw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "42146e4d-b802-46ee-ca99-9b8d9e68ffdb"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import time\n",
        "\n",
        "# Hyperparameters\n",
        "param_grid = [{'max_depth': [None, 10], 'n_estimators': [100], \n",
        "               'max_features': ['sqrt', None]}]\n",
        "\n",
        "# Create model and fit to training data.\n",
        "# Perform grid search CV to find the best hyperparameters.\n",
        "start_time = time.time()\n",
        "rf = RandomForestClassifier(n_jobs=-1)\n",
        "rf = GridSearchCV(rf, param_grid)\n",
        "rf.fit(train_x, train_y)\n",
        "\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "\n",
        "# Print model with chosen hyperparameters\n",
        "print(rf)\n",
        "\n",
        "# Predict on test data\n",
        "rf_predict = rf.predict(test_x)\n",
        "\n",
        "# Get accuracy\n",
        "rf_acc = (rf_predict == test_y).mean()\n",
        "print(rf_acc)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--- 10637.147904396057 seconds ---\n",
            "GridSearchCV(cv=None, error_score='raise',\n",
            "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
            "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
            "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "            min_samples_leaf=1, min_samples_split=2,\n",
            "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=-1,\n",
            "            oob_score=False, random_state=None, verbose=0,\n",
            "            warm_start=False),\n",
            "       fit_params=None, iid=True, n_jobs=1,\n",
            "       param_grid=[{'max_depth': [None, 10], 'n_estimators': [100], 'max_features': ['sqrt', None]}],\n",
            "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
            "       scoring=None, verbose=0)\n",
            "0.6263\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zKeYx7-vv62I",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Original features"
      ]
    },
    {
      "metadata": {
        "id": "XHL5YxsEGXEq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "5b1db915-e65b-4fa5-def3-11a022563a54"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import time\n",
        "\n",
        "# Hyperparameters\n",
        "param_grid = [{'max_depth': [None, 10], 'n_estimators': [100],\n",
        "               'max_features': ['sqrt', None]}]\n",
        "\n",
        "# Create model and fit to training data.\n",
        "# Perform grid search CV to find the best hyperparameters.\n",
        "start_time = time.time()\n",
        "\n",
        "rf_orig = RandomForestClassifier(n_jobs=-1)\n",
        "rf_orig = GridSearchCV(rf_orig, param_grid)\n",
        "rf_orig.fit(train_x_orig, train_y)\n",
        "\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "\n",
        "# print model with chosen parameters\n",
        "print(rf_orig)\n",
        "\n",
        "# Predict on test data\n",
        "rf_predict_orig = rf_orig.predict(test_x_orig)\n",
        "\n",
        "# Get accuracy\n",
        "rf_acc_orig = (rf_predict_orig == test_y).mean()\n",
        "print(rf_acc_orig)\n",
        "# 0.8774"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--- 6703.958931207657 seconds ---\n",
            "GridSearchCV(cv=None, error_score='raise',\n",
            "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
            "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
            "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "            min_samples_leaf=1, min_samples_split=2,\n",
            "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=-1,\n",
            "            oob_score=False, random_state=None, verbose=0,\n",
            "            warm_start=False),\n",
            "       fit_params=None, iid=True, n_jobs=1,\n",
            "       param_grid=[{'max_depth': [None, 10], 'n_estimators': [100], 'max_features': ['sqrt', None]}],\n",
            "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
            "       scoring=None, verbose=0)\n",
            "0.8763\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}